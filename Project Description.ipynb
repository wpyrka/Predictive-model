{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e715e3-bcaf-4412-9975-395c2656cc0d",
   "metadata": {
    "id": "80e715e3-bcaf-4412-9975-395c2656cc0d"
   },
   "source": [
    "# Progetto - Complementi di Basi di Dati A.A. 2023/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55b63c",
   "metadata": {},
   "source": [
    "# Libraries Used in the Project\n",
    "\n",
    "### pandas\n",
    "\n",
    "**Purpose:** pandas is used for data manipulation and analysis, to read data from CSV files, perform data transformation and cleaning operations\n",
    "\n",
    "### joblib\n",
    "\n",
    "**Purpose:** joblib is a library used for saving and loading Python objects, including machine learning models. Here is used to save the model after training so that it can be easily loaded and used for predictions later.\n",
    "\n",
    "### fuzzywuzzy\n",
    "\n",
    "**Purpose:** fuzzywuzzy is a Python library that provides fuzzy string matching capabilities. It is used here to correct invalid entries in the data.\n",
    "\n",
    "### SQLAlchemy\n",
    "\n",
    "**Purpose:** SQLAlchemy is used to connect to a PostgreSQL database, load data from it and save prediction results back to the database\n",
    "\n",
    "### scikit-learn (sklearn)\n",
    "\n",
    "**Purpose:** is a machine learning library in Python that provides various tools for machine learning tasks. Here is used for data preprocessing, model training model evaluation and making predictions.\n",
    "\n",
    "### scikit-optimize\n",
    "\n",
    "**Purpose:** scikit-optimize is a library for sequential model-based optimization in Python. It is used to create hyperparameters for the model used in scikit-learn using a Bayesian optimization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314372a",
   "metadata": {},
   "source": [
    "# Code explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e590ca",
   "metadata": {},
   "source": [
    "## Functions Used in the Code\n",
    "\n",
    "- **get_NA_columns(data_description)**: Identifies columns where 'NA' is a valid value according to the provided data description and returns a list of such columns. This is done to differentiate between real NaN and  Na values that pandas automatically considers null.\n",
    "\n",
    "- **validate_and_correct_value(column, value, data_description)**: Validates and corrects the value if it’s not valid for the specific column. It also applies a fuzzySearch if it thinks the value could be a typo.\n",
    "\n",
    "- **clean_and_validate_data(dataFrame, data_description, columns_to_group)**: Iterates through every column of the DataFrame to check if the values are all correct, if it finds anything wrong the data is sent to `validate_and_correct_value`.\n",
    "\n",
    "- **save_model(best_model, model_file)**: Saves the best-trained model to a file using `joblib`.\n",
    "\n",
    "- **load_model(model_file)**: Loads a trained model from a file using `joblib`.\n",
    "\n",
    "- **load_data(engine, table_name, data_description)**: Loads data from a specified table in the PostgreSQL database and replaces NaN values in columns where 'NA' is a valid value according to the data description.\n",
    "\n",
    "- **prepare_data_for_prediction(data)**: Transforms categorical string variables into numerical values using either a `LabelEncoder` or the `ordinal_encodings`, preparing the data for machine learning models.\n",
    "\n",
    "- **find_best_params(data)**: Finds the best parameters for the predictive model using Bayesian optimization.\n",
    "\n",
    "- **train_model(data, model_file)**: Trains a predictive model using the generated hyperparameters from the `find_best_params` function.\n",
    "\n",
    "- **predict_results(model, data)**: Uses the trained model to make predictions on the test data and returns a DataFrame containing the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d563b2",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset used for this project is the Ames Housing Dataset, sourced from the Kaggle competition \"House Prices: Advanced Regression Techniques\". This dataset offers a comprehensive collection of data on residential property sales in Ames, Iowa in USA, and is widely utilized in the data science community due to its rich and diverse features, making it ideal for various types of data analysis and modeling projects.\n",
    "\n",
    "The dataset is divided into two main files: ‘train.csv’ and ‘test.csv’. The training set includes 1460 observations with 81 features each and the test set contains 1459 observations with 80 features.\n",
    "\n",
    "The features in this dataset span a broad range of categories, each contributing uniquely to the property's overall value. These categories include property descriptors, location features, house features, rooms and facilities, external features and miscellaneous features. Each feature category provides a different dimension of information about the properties, contributing to a holistic view necessary for accurate price prediction.\n",
    "\n",
    "## Why did we choose this dataset?\n",
    "\n",
    "Firstly, its richness and diversity make it ideal for a predictive modeling project. The dataset includes a comprehensive set of features that cover various aspects of a residential property, providing ample opportunities for extensive exploration and feature engineering. The dataset is well-documented and popular within the data science community. This popularity means there is a wealth of resources, discussions and prior work available, which can be helpful for reference and benchmarking. It provides a solid foundation for learning and applying advanced regression techniques. It is well-structured and of a manageable size, providing enough data to train robust models without requiring extensive computational resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11ed1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'test_db' deleted successfully.\n",
      "Database 'test_db' successfully created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Connection parameters for the PostgreSQL server\n",
    "db_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=\"postgres\",\n",
    "    password=\"unimib\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    database=f\"postgres\"\n",
    ")\n",
    "\n",
    "# Database name\n",
    "new_db_name = 'test_db'\n",
    "\n",
    "try:\n",
    "    # Creation of the engine for the connection to the database\n",
    "    engine = create_engine(db_url, isolation_level=\"AUTOCOMMIT\")\n",
    "    with engine.connect() as conn:\n",
    "        # Check if the database exists\n",
    "        result = conn.execute(text(f\"SELECT 1 FROM pg_database WHERE datname = '{new_db_name}'\"))\n",
    "        exists = result.scalar() is not None\n",
    "\n",
    "        # If it does eliminate it\n",
    "        if exists:\n",
    "            conn.execute(text(f\"DROP DATABASE {new_db_name} WITH (FORCE)\"))\n",
    "            print(f\"Database '{new_db_name}' deleted successfully.\")\n",
    "\n",
    "        # Create the database\n",
    "        conn.execute(text(f\"CREATE DATABASE {new_db_name}\"))\n",
    "        print(f\"Database '{new_db_name}' successfully created.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cc6fc-4b69-49af-8670-b8f16840ec5c",
   "metadata": {
    "id": "2e9cc6fc-4b69-49af-8670-b8f16840ec5c"
   },
   "source": [
    "# CRUD analysis\n",
    "CRUD stands for create, read, update, and delete. These are the four basic operations that can be performed on data in a database:\n",
    "\n",
    "- **create** - inserting new records into the database\n",
    "- **read** - retrieving existing records from the database\n",
    "- **update** - modifying existing records in the database\n",
    "- **delete** - removing records from the database\n",
    "\n",
    "This analysis involves examining these operations to understand how data flows through the system and how it is manipulated. This analysis helps in designing, optimizing, and maintaining database systems.\n",
    "\n",
    "**In our project:**\n",
    "\n",
    "- **create:** During the initial phase of the project, we used the `create_engine` function from SQLAlchemy to establish a connection to the PostgreSQL database and then inserted the cleaned and prepared data into the database.\n",
    "\n",
    "- **read:** Throughout the project, we frequently retrieved data from the PostgreSQL database for analysis and modeling. We used Pandas' `read_sql` function to execute SQL queries and load data into data frames.\n",
    "\n",
    "- **update:** We implemented data validation and correction functions to ensure data integrity. When invalid or missing values were detected, they were corrected or replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea350df2",
   "metadata": {},
   "source": [
    "### Connect to PostgreSQL\n",
    "- **Connect to PostgreSQL**: Establishes a connection to a PostgreSQL database named `test_db` running locally with username `postgres` and password `unimib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8a8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:unimib@localhost/test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a0587",
   "metadata": {},
   "source": [
    "## Create Table\n",
    "- **Create Table**: Creates a DataFrame `dataFrame` with columns `Id`, `A`, `B`, and `C`, and saves it to a table named `test_table` in the PostgreSQL database. If the table already exists, it will be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899442f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'test_table' successfully created.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Id': [1, 2, 3],\n",
    "    'A': ['foo', 'bar', 'baz'],\n",
    "    'B': [True, False, True],\n",
    "    'C': [float('NaN'), float('NaN'), float('NaN')]\n",
    "}\n",
    "\n",
    "dataFrame = pd.DataFrame(data)\n",
    "table_name = \"test_table\"\n",
    "dataFrame.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "print(f\"Table '{table_name}' successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26566ed2",
   "metadata": {},
   "source": [
    "### Read from Table\n",
    "\n",
    "- **Read from Table**: Executes SQL queries to fetch all rows from `test_table` and prints the resulting DataFrame, and another query that fetches rows where `Id` is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6e9084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    A      B     C\n",
      "0   1  foo   True  None\n",
      "1   2  bar  False  None\n",
      "2   3  baz   True  None\n"
     ]
    }
   ],
   "source": [
    "table_name = \"test_table\"\n",
    "query = f'SELECT * FROM {table_name}'\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749df63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    A     B     C\n",
      "0   1  foo  True  None\n"
     ]
    }
   ],
   "source": [
    "table_name = \"test_table\"\n",
    "query = f'SELECT * FROM {table_name} WHERE \"Id\" = 1'\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355f270",
   "metadata": {},
   "source": [
    "## Update Table\n",
    "- **Update Table**: Updates the `C` column in `test_table` based on values in the `data` dictionary using SQL `UPDATE` statements. Uses SQLAlchemy's `text` construct to write SQL queries with parameters and executes them through the PostgreSQL connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15e3e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    A      B    C\n",
      "0   1  foo   True  1.2\n",
      "1   2  bar  False  3.4\n",
      "2   3  baz   True  8.0\n"
     ]
    }
   ],
   "source": [
    "table_name = \"test_table\"\n",
    "data = {\n",
    "    'Id': [1, 2, 3],\n",
    "    'C': [1.2, 3.4, 8.0]\n",
    "}\n",
    "\n",
    "dataFrame = pd.DataFrame(data)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    for index, row in dataFrame.iterrows():\n",
    "        update_query = text(f\"\"\"\n",
    "            UPDATE {table_name}\n",
    "            SET \"C\" = :c\n",
    "            WHERE \"Id\" = :id\n",
    "        \"\"\")\n",
    "        \n",
    "        params = {\n",
    "            'id': row['Id'],\n",
    "            'c': row['C']\n",
    "        }\n",
    "        \n",
    "        connection.execute(update_query, params)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "table_name = \"test_table\"\n",
    "query = f'SELECT * FROM {table_name}'\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0aa70",
   "metadata": {},
   "source": [
    "## Delete Table\n",
    "- **Delete Table**: Updates the `C` column in `test_table` based on values in the `data` dictionary using SQL `UPDATE` statements. Uses SQLAlchemy's `text` construct to write SQL queries with parameters and executes them through the PostgreSQL connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a020f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'test_db' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "table_name = \"test_table\"\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    drop_query = text(f'DROP TABLE IF EXISTS {table_name}')\n",
    "    connection.execute(drop_query)\n",
    "    connection.commit()\n",
    "    print(f\"Table '{new_db_name}' deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8048fc8",
   "metadata": {},
   "source": [
    "# Database Systems Comparison\n",
    "\n",
    "### PostgreSQL\n",
    "\n",
    "PostgreSQL is a powerful and open-source relational database system known for its reliability and stability. It excels in handling complex queries and supports a wide range of data types. We chose PostgreSQL for our project due to its robustness, which is essential when dealing with large datasets. It seamlessly integrates with Python, facilitating smooth data loading and management. Overall, PostgreSQL is a solid choice for storing and analyzing our housing data.\n",
    "\n",
    "### Other DBMS\n",
    "\n",
    "- **MongoDB:** MongoDB is a NoSQL, document-oriented database renowned for its flexibility and scalability. Unlike traditional relational databases, MongoDB stores data in JSON-like documents, allowing for dynamic schema design and easy handling of unstructured or semi-structured data. It is highly suitable for projects requiring rapid development and frequent data format changes. MongoDB's horizontal scalability enables it to handle large volumes of data and high demand scenarios effectively.\n",
    "\n",
    "- **Neo4j:** Neo4j is a graph database management system designed for storing and querying data in graph structures. It utilizes nodes, relationships, and properties to represent and store data, making it ideal for applications with complex and interconnected data, such as social networks and recommendation engines. Neo4j's optimized graph storage and processing engine provide efficient querying and traversal capabilities that are challenging to achieve with traditional relational databases.\n",
    "\n",
    "- **MySQL:** MySQL is an open-source relational database management system widely used for web applications and data warehousing. It follows the traditional relational model, organizing data into tables with rows and columns. MySQL supports standard SQL for querying and managing data, making it popular for applications needing to manage complex transactions and ensure data consistency. Known for its reliability, ease of use, and strong community support, MySQL offers various storage engines to cater to diverse performance and feature requirements.\n",
    "\n",
    "\n",
    "| Feature          | PostgreSQL                                                   | MongoDB                                                       | Neo4j                                                         | MySQL                                                         |\n",
    "|------------------|--------------------------------------------------------------|---------------------------------------------------------------|---------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| Data Model       | Relational (tables)                                          | Document-oriented (JSON-like documents)                        | Graphs (nodes, relationships)                                  | Relational (tables)                                           |\n",
    "| Query Language   | SQL                                                          | JSON-based Query Language                                      | Cypher                                                        | SQL                                                           |\n",
    "| Schema           | Fixed schema, requires migration for changes                 | Flexible schema, dynamic and can evolve over time              | Schema-less for nodes and relationships                        | Fixed schema, requires migration for changes                  |\n",
    "| Scalability      | Vertical and horizontal scaling                              | Horizontal scaling                                             | Horizontal scaling                                             | Vertical and horizontal scaling                               |\n",
    "| Use Cases        | Financial systems, enterprise applications, complex queries  | Content management, IoT applications, high write loads         | Social networks, recommendation engines, complex relationships | Web applications, content management systems, reliable transactional systems |\n",
    "| Data Integrity   | High                                                         | Moderate                                                      | Moderate                                                      | High                                                          |\n",
    "| Performance      | High performance, especially for read-heavy operations       | High performance for write-heavy and unstructured data workloads | Optimized for graph operations and traversals                  | High performance, especially for read-heavy operations        |\n",
    "| ACID Compliance  | Yes                                                          | Yes (with limitations)                                         | Yes                                                           | Yes                                                           |\n",
    "| Strengths        | Strong consistency and reliability, rich SQL features        | Flexible schema design, high scalability, good for hierarchical data | Intuitive graph traversal, efficient for connected data       | Wide adoption, strong community support, good for reliable transaction processing |\n",
    "| Weaknesses       | Can be complex to scale horizontally, may be slower with very large datasets | Less efficient for complex queries compared to SQL databases   | Less mature ecosystem compared to relational DBs, complexity in scaling | Limited support for complex data types, less performant for large-scale write operations |\n",
    "\n",
    "To summarize, PostgreSQL was chosen for this project for several compelling reasons:\n",
    "\n",
    "- **Integration with Pandas:** PostgreSQL seamlessly integrates with the Pandas library, facilitating efficient data manipulation and analysis tasks throughout the project.\n",
    "\n",
    "- **Data Integrity and Reliability:** PostgreSQL ensures robust data integrity with ACID compliance, which is crucial for maintaining transactional consistency and ensuring accurate forecasting of housing prices.\n",
    "\n",
    "- **Flexibility:** PostgreSQL offers extensive support for various data types and advanced features, providing the flexibility to handle diverse datasets and complex data structures effectively.\n",
    "\n",
    "- **Extensibility:** PostgreSQL's extensible nature allows for the integration of custom functions, data types, and operators, enabling tailored solutions to specific project requirements.\n",
    "\n",
    "- **Performance:** PostgreSQL is recognized for its high performance, particularly in read-heavy operations. This capability is advantageous given the project's large dataset and complex query requirements.\n",
    "\n",
    "- **Strong Community and Documentation:** PostgreSQL benefits from a vibrant community and comprehensive documentation, ensuring ample resources for support, best practices, and continuous learning throughout the project's lifecycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719e4de-2f17-4db9-a8d5-27ac98d43b11",
   "metadata": {
    "id": "1719e4de-2f17-4db9-a8d5-27ac98d43b11"
   },
   "source": [
    "# Query and Timing\n",
    "\n",
    "Here are some examples of different queries using our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Connect to postgreSQL\n",
    "engine = create_engine('postgresql://postgres:unimib@localhost/ames_housing_db')\n",
    "\n",
    "test_table = \"housing_data_test\"\n",
    "train_table = \"housing_data_train\"\n",
    "results_table = \"housing_data_prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc7ab0",
   "metadata": {},
   "source": [
    "0. **Query printing the whole database:**\n",
    "    - A normal select query without any condition asking for the entire table (`train_table`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ebc6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0        1          60       RL         65.0     8450   Pave    NA      Reg   \n",
      "1        2          20       RL         80.0     9600   Pave    NA      Reg   \n",
      "2        3          60       RL         68.0    11250   Pave    NA      IR1   \n",
      "3        4          70       RL         60.0     9550   Pave    NA      IR1   \n",
      "4        5          60       RL         84.0    14260   Pave    NA      IR1   \n",
      "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
      "1455  1456          60       RL         62.0     7917   Pave    NA      Reg   \n",
      "1456  1457          20       RL         85.0    13175   Pave    NA      Reg   \n",
      "1457  1458          70       RL         66.0     9042   Pave    NA      Reg   \n",
      "1458  1459          20       RL         68.0     9717   Pave    NA      Reg   \n",
      "1459  1460          20       RL         75.0     9937   Pave   nan      Reg   \n",
      "\n",
      "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
      "0            Lvl    AllPub  ...        0     NA  MnPrv          NA       0   \n",
      "1            Lvl    AllPub  ...        0     NA     NA        Gar2       0   \n",
      "2            Lvl    AllPub  ...        0     NA  MnPrv          NA       0   \n",
      "3            Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "4            Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
      "1455         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "1456         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "1457         Lvl    AllPub  ...        0     NA  MnPrv        Shed    2500   \n",
      "1458         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "1459         Lvl    AllPub  ...        0    nan    nan         nan       0   \n",
      "\n",
      "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0         2   2008        WD         Normal   208500.0  \n",
      "1         5   2007        WD         Normal   181500.0  \n",
      "2         9   2008        WD         Normal   223500.0  \n",
      "3         2   2006        WD        Abnorml   140000.0  \n",
      "4        12   2008        WD         Normal   250000.0  \n",
      "...     ...    ...       ...            ...        ...  \n",
      "1455      8   2007        WD         Normal   175000.0  \n",
      "1456      2   2010        WD         Normal   210000.0  \n",
      "1457      5   2010        WD         Normal   266500.0  \n",
      "1458      4   2010        WD         Normal   142125.0  \n",
      "1459      6   2008        WD         Normal   147500.0  \n",
      "\n",
      "[1460 rows x 81 columns]\n",
      "Time elapsed: 0.0582s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'SELECT * FROM {train_table}'\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61949495",
   "metadata": {},
   "source": [
    "1. **Query aggregating data:**\n",
    "   - Calculate the average sale price (`SalePrice`) for each street condition (`Street`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8cb22a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Street   avgsaleprice\n",
      "0   Pave  181130.538514\n",
      "1   Grvl  130190.500000\n",
      "Time elapsed: 0.0020s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'''\n",
    "   SELECT \"Street\", AVG(\"SalePrice\") AS AvgSalePrice\n",
    "   FROM {train_table}\n",
    "   GROUP BY \"Street\";\n",
    "'''\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913e5ec",
   "metadata": {},
   "source": [
    "2. **Query with nested query:**\n",
    "   - Find the house with the highest sale price (`SalePrice`) and its neighborhood (`Neighborhood`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1599a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Neighborhood  SalePrice\n",
      "0      NoRidge   755000.0\n",
      "Time elapsed: 0.0024s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'''\n",
    "   SELECT \"Neighborhood\", \"SalePrice\"\n",
    "   FROM {train_table}\n",
    "   WHERE \"SalePrice\" = (SELECT MAX(\"SalePrice\") FROM {train_table});\n",
    "'''\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b64cf2",
   "metadata": {},
   "source": [
    "3. **Query with multiple operations:**\n",
    "   - Compute the absolute difference and percentage error between predicted sale price (`SalePrice_predicted`) and actual sale price (`SalePrice_actual`) for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8fd625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  SalePrice_actual SalePrice_predicted abserror percenterror\n",
      "0     1461     169277.052498                None     None         None\n",
      "1     1462     187758.393989                None     None         None\n",
      "2     1463     183583.683570                None     None         None\n",
      "3     1464     179317.477511                None     None         None\n",
      "4     1465     150730.079977                None     None         None\n",
      "...    ...               ...                 ...      ...          ...\n",
      "1454  2915     167081.220949                None     None         None\n",
      "1455  2916     164788.778231                None     None         None\n",
      "1456  2917     219222.423400                None     None         None\n",
      "1457  2918     184924.279659                None     None         None\n",
      "1458  2919     187741.866657                None     None         None\n",
      "\n",
      "[1459 rows x 5 columns]\n",
      "Time elapsed: 0.0035s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'''\n",
    "    SELECT\n",
    "        \"Id\",\n",
    "        \"SalePrice_actual\",\n",
    "        \"SalePrice_predicted\",\n",
    "        ABS(\"SalePrice_actual\" - \"SalePrice_predicted\") AS AbsError,\n",
    "        ABS((\"SalePrice_actual\" - \"SalePrice_predicted\") / \"SalePrice_actual\") * 100 AS PercentError\n",
    "    FROM {results_table};\n",
    "'''\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9163c",
   "metadata": {},
   "source": [
    "4. **Complex query with JOIN and aggregation:**\n",
    "   - Calculate the average absolute error (`AvgAbsError`) and average percentage error (`AvgPercentError`) for each neighborhood (`Neighborhood`) by joining the training dataset with the results dataset on matching IDs (`Id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa5258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Neighborhood, avgabserror, avgpercenterror]\n",
      "Index: []\n",
      "Time elapsed: 0.0020s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'''\n",
    "    SELECT train.\"Neighborhood\",\n",
    "        AVG(result.\"AbsError\") AS AvgAbsError,\n",
    "        AVG(result.\"PercentError\") AS AvgPercentError\n",
    "    FROM {train_table} train\n",
    "    JOIN {results_table} result ON train.\"Id\" = result.\"Id\"\n",
    "    GROUP BY train.\"Neighborhood\"\n",
    "    ORDER BY AvgPercentError DESC;\n",
    "'''\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c0708",
   "metadata": {},
   "source": [
    "5. **Query using aggregate functions and filters:**\n",
    "   - Count the number of houses with a pool (`PoolArea > 0`) for each zoning type (`MSZoning`) in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057edda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning  numhouseswithpool\n",
      "0       RL                  6\n",
      "Time elapsed: 0.0030s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = f'''\n",
    "    SELECT \"MSZoning\", COUNT(*) AS NumHousesWithPool\n",
    "    FROM {test_table}\n",
    "    WHERE \"PoolArea\" > 0\n",
    "    GROUP BY \"MSZoning\";\n",
    "'''\n",
    "dataFrame = pd.read_sql(query, engine)\n",
    "time_elapsed = time.time() - start_time\n",
    "print(dataFrame)\n",
    "print(f\"Time elapsed: {time_elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4671ea",
   "metadata": {},
   "source": [
    "When working with databases, especially with smaller dimensions like those containing just over 1500 records per table, the performance of queries can vary significantly depending on how data is retrieved and processed.\n",
    "\n",
    "In this scenario, simple SELECT queries execute quickly, typically completing in milliseconds (e.g., from 0.0035 seconds to 0.0300 seconds). However, as the size of data or complexity of operations increases, the time taken can become more substantial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99847a17-189b-4675-94c9-ad5c142c23b2",
   "metadata": {
    "id": "99847a17-189b-4675-94c9-ad5c142c23b2"
   },
   "source": [
    "### Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a1e4f-0671-4d91-b429-70102da08266",
   "metadata": {
    "id": "229a1e4f-0671-4d91-b429-70102da08266"
   },
   "source": [
    "**Roberto** **Boccaccio** **869135**.\n",
    "\n",
    "**Weronika** **Pyrka** **908115**.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
